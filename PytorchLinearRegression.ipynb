{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyORJLudEwyxaFEHcKpM78nq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Somesh140/Pytorch-basics/blob/main/PytorchLinearRegression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SASHo_ovbRH6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87c23cfd-3733-4bb5-f74d-c5e6f793844e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Jun 20 13:15:34 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   39C    P8     9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch"
      ],
      "metadata": {
        "id": "EutFA8QbiWH0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Input data\n",
        "input = np.array([[32,55,77],\n",
        "                  [31,75,57],\n",
        "                  [52,55,77],\n",
        "                  [22,100,87],\n",
        "                  [62,80,77]], dtype = 'float32' )\n",
        "input"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8Z-0vckryVd",
        "outputId": "2548391b-1705-416a-9953-52b6fe1bb85b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 32.,  55.,  77.],\n",
              "       [ 31.,  75.,  57.],\n",
              "       [ 52.,  55.,  77.],\n",
              "       [ 22., 100.,  87.],\n",
              "       [ 62.,  80.,  77.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# target\n",
        "target = np.array([[32,66],\n",
        "                   [52,76],\n",
        "                   [102,55],\n",
        "                   [32,26],\n",
        "                   [99,100]\n",
        "                   ],dtype='float32')\n",
        "target"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-h_GiBcCr_xe",
        "outputId": "3261bc06-b5ce-4427-88ba-4fe697096df1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 32.,  66.],\n",
              "       [ 52.,  76.],\n",
              "       [102.,  55.],\n",
              "       [ 32.,  26.],\n",
              "       [ 99., 100.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = torch.from_numpy(input)\n",
        "targets = torch.from_numpy(target)\n",
        "print(inputs,targets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w9FjJMmSTz7F",
        "outputId": "865b53c3-aa83-4d91-a59e-b6688ee831a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 32.,  55.,  77.],\n",
            "        [ 31.,  75.,  57.],\n",
            "        [ 52.,  55.,  77.],\n",
            "        [ 22., 100.,  87.],\n",
            "        [ 62.,  80.,  77.]]) tensor([[ 32.,  66.],\n",
            "        [ 52.,  76.],\n",
            "        [102.,  55.],\n",
            "        [ 32.,  26.],\n",
            "        [ 99., 100.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# weights and biases\n",
        "w = torch.randn(2,3, requires_grad = True) # 2 output column and 3 input columns\n",
        "b = torch.randn(2, requires_grad= True) # 2 output columns\n",
        "\n",
        "print(w)\n",
        "print(b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZuR8IbduXSrZ",
        "outputId": "aeacba20-c9d0-4178-df36-89cd6d541b9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.1278,  0.9046,  0.4064],\n",
            "        [-2.0081, -0.7997,  0.9862]], requires_grad=True)\n",
            "tensor([-0.3955, -0.1073], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def model(x):\n",
        "  return x@w.t() + b"
      ],
      "metadata": {
        "id": "cKbTYvh9YVz8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds = model(inputs)\n",
        "print(preds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qTrl4Zbi0VQm",
        "outputId": "97c18d3c-7da3-4acb-e001-ec47780e27cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[  76.5648,  -32.4067],\n",
            "        [  86.6561,  -66.1174],\n",
            "        [  74.0091,  -72.5678],\n",
            "        [ 122.6146,  -38.4496],\n",
            "        [  95.3466, -112.6405]], grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "targets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jOXx8rjE0sqy",
        "outputId": "76178bf5-8c19-4dd5-b325-3a7121a870b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 32.,  66.],\n",
              "        [ 52.,  76.],\n",
              "        [102.,  55.],\n",
              "        [ 32.,  26.],\n",
              "        [ 99., 100.]])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# loss\n",
        "def MSE(t1,t2):\n",
        "  diff = t1 -t2\n",
        "  return torch.sum(diff*diff)/diff.numel()"
      ],
      "metadata": {
        "id": "JZIniFRV0vlE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compute loss\n",
        "loss = MSE(preds, targets)\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eocpElPv1qay",
        "outputId": "7f0780c6-4525-4504-adea-14db2c9413b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(10771.9434, grad_fn=<DivBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute gradient\n",
        "loss.backward()"
      ],
      "metadata": {
        "id": "lnSUxoB710Lt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(w)\n",
        "print(w.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E2FPvnyU3ujc",
        "outputId": "aef1151b-1687-4bd6-8527-8ef7f8312970"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.1278,  0.9046,  0.4064],\n",
            "        [-2.0081, -0.7997,  0.9862]], requires_grad=True)\n",
            "tensor([[  562.3798,  2455.9924,  2170.7498],\n",
            "        [-5757.9565, -9308.7207, -9496.2324]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad(): # no_grad to disable gradient calculation\n",
        "  w -= w.grad * 1e-5\n",
        "  b -= b.grad * 1e-5"
      ],
      "metadata": {
        "id": "1aenW-tY30E5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(w)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GiYsHp175u5y",
        "outputId": "3277e137-72ad-46b4-d780-5cefccc33306"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.1334,  0.8801,  0.3847],\n",
            "        [-1.9505, -0.7066,  1.0812]], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preds = model(inputs)"
      ],
      "metadata": {
        "id": "K4bUw52C6NTJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss = MSE(preds, targets)\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SyMVOGV57gMp",
        "outputId": "09956a0c-1e2d-4e84-a484-c06c4978b9f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(8701.4629, grad_fn=<DivBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training\n",
        "n = 400\n",
        "for i in range(n):\n",
        "  preds = model(inputs)\n",
        "  loss = MSE(preds, targets)\n",
        "  loss.backward()\n",
        "  with torch.no_grad(): # no_grad to disable gradient calculation\n",
        "    w -= w.grad * 1e-5\n",
        "    b -= b.grad * 1e-5\n",
        "\n",
        "  w.grad.zero_() # to clear the previous gradient values\n",
        "  b.grad.zero_()\n",
        "\n",
        "  print(f\"EPOCHS: {i}/{n}---------Loss: {loss}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D_KtTqgC7jCv",
        "outputId": "58ec3403-b7f8-4602-c500-34ba00e53350"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCHS: 0/400---------Loss: 1327.6907958984375\n",
            "EPOCHS: 1/400---------Loss: 1322.6842041015625\n",
            "EPOCHS: 2/400---------Loss: 1317.705078125\n",
            "EPOCHS: 3/400---------Loss: 1312.752685546875\n",
            "EPOCHS: 4/400---------Loss: 1307.8275146484375\n",
            "EPOCHS: 5/400---------Loss: 1302.9290771484375\n",
            "EPOCHS: 6/400---------Loss: 1298.0570068359375\n",
            "EPOCHS: 7/400---------Loss: 1293.211669921875\n",
            "EPOCHS: 8/400---------Loss: 1288.3924560546875\n",
            "EPOCHS: 9/400---------Loss: 1283.599365234375\n",
            "EPOCHS: 10/400---------Loss: 1278.832275390625\n",
            "EPOCHS: 11/400---------Loss: 1274.0911865234375\n",
            "EPOCHS: 12/400---------Loss: 1269.375732421875\n",
            "EPOCHS: 13/400---------Loss: 1264.6856689453125\n",
            "EPOCHS: 14/400---------Loss: 1260.021240234375\n",
            "EPOCHS: 15/400---------Loss: 1255.3814697265625\n",
            "EPOCHS: 16/400---------Loss: 1250.7672119140625\n",
            "EPOCHS: 17/400---------Loss: 1246.177490234375\n",
            "EPOCHS: 18/400---------Loss: 1241.6129150390625\n",
            "EPOCHS: 19/400---------Loss: 1237.0728759765625\n",
            "EPOCHS: 20/400---------Loss: 1232.5572509765625\n",
            "EPOCHS: 21/400---------Loss: 1228.0660400390625\n",
            "EPOCHS: 22/400---------Loss: 1223.599365234375\n",
            "EPOCHS: 23/400---------Loss: 1219.1561279296875\n",
            "EPOCHS: 24/400---------Loss: 1214.737060546875\n",
            "EPOCHS: 25/400---------Loss: 1210.341796875\n",
            "EPOCHS: 26/400---------Loss: 1205.9703369140625\n",
            "EPOCHS: 27/400---------Loss: 1201.6220703125\n",
            "EPOCHS: 28/400---------Loss: 1197.2972412109375\n",
            "EPOCHS: 29/400---------Loss: 1192.995849609375\n",
            "EPOCHS: 30/400---------Loss: 1188.7169189453125\n",
            "EPOCHS: 31/400---------Loss: 1184.4615478515625\n",
            "EPOCHS: 32/400---------Loss: 1180.228759765625\n",
            "EPOCHS: 33/400---------Loss: 1176.018798828125\n",
            "EPOCHS: 34/400---------Loss: 1171.8310546875\n",
            "EPOCHS: 35/400---------Loss: 1167.665771484375\n",
            "EPOCHS: 36/400---------Loss: 1163.5230712890625\n",
            "EPOCHS: 37/400---------Loss: 1159.4024658203125\n",
            "EPOCHS: 38/400---------Loss: 1155.3035888671875\n",
            "EPOCHS: 39/400---------Loss: 1151.2269287109375\n",
            "EPOCHS: 40/400---------Loss: 1147.1717529296875\n",
            "EPOCHS: 41/400---------Loss: 1143.13818359375\n",
            "EPOCHS: 42/400---------Loss: 1139.12646484375\n",
            "EPOCHS: 43/400---------Loss: 1135.1357421875\n",
            "EPOCHS: 44/400---------Loss: 1131.166259765625\n",
            "EPOCHS: 45/400---------Loss: 1127.2181396484375\n",
            "EPOCHS: 46/400---------Loss: 1123.291015625\n",
            "EPOCHS: 47/400---------Loss: 1119.384521484375\n",
            "EPOCHS: 48/400---------Loss: 1115.498779296875\n",
            "EPOCHS: 49/400---------Loss: 1111.634033203125\n",
            "EPOCHS: 50/400---------Loss: 1107.78955078125\n",
            "EPOCHS: 51/400---------Loss: 1103.9652099609375\n",
            "EPOCHS: 52/400---------Loss: 1100.161376953125\n",
            "EPOCHS: 53/400---------Loss: 1096.3780517578125\n",
            "EPOCHS: 54/400---------Loss: 1092.6142578125\n",
            "EPOCHS: 55/400---------Loss: 1088.8704833984375\n",
            "EPOCHS: 56/400---------Loss: 1085.146728515625\n",
            "EPOCHS: 57/400---------Loss: 1081.4425048828125\n",
            "EPOCHS: 58/400---------Loss: 1077.758056640625\n",
            "EPOCHS: 59/400---------Loss: 1074.092529296875\n",
            "EPOCHS: 60/400---------Loss: 1070.4466552734375\n",
            "EPOCHS: 61/400---------Loss: 1066.820068359375\n",
            "EPOCHS: 62/400---------Loss: 1063.2125244140625\n",
            "EPOCHS: 63/400---------Loss: 1059.6239013671875\n",
            "EPOCHS: 64/400---------Loss: 1056.0543212890625\n",
            "EPOCHS: 65/400---------Loss: 1052.50341796875\n",
            "EPOCHS: 66/400---------Loss: 1048.97119140625\n",
            "EPOCHS: 67/400---------Loss: 1045.4573974609375\n",
            "EPOCHS: 68/400---------Loss: 1041.9622802734375\n",
            "EPOCHS: 69/400---------Loss: 1038.485595703125\n",
            "EPOCHS: 70/400---------Loss: 1035.0269775390625\n",
            "EPOCHS: 71/400---------Loss: 1031.5865478515625\n",
            "EPOCHS: 72/400---------Loss: 1028.1639404296875\n",
            "EPOCHS: 73/400---------Loss: 1024.7593994140625\n",
            "EPOCHS: 74/400---------Loss: 1021.3727416992188\n",
            "EPOCHS: 75/400---------Loss: 1018.0037231445312\n",
            "EPOCHS: 76/400---------Loss: 1014.65234375\n",
            "EPOCHS: 77/400---------Loss: 1011.3182373046875\n",
            "EPOCHS: 78/400---------Loss: 1008.0017700195312\n",
            "EPOCHS: 79/400---------Loss: 1004.7025146484375\n",
            "EPOCHS: 80/400---------Loss: 1001.4205322265625\n",
            "EPOCHS: 81/400---------Loss: 998.1556396484375\n",
            "EPOCHS: 82/400---------Loss: 994.9078369140625\n",
            "EPOCHS: 83/400---------Loss: 991.6767578125\n",
            "EPOCHS: 84/400---------Loss: 988.4625244140625\n",
            "EPOCHS: 85/400---------Loss: 985.2648315429688\n",
            "EPOCHS: 86/400---------Loss: 982.0841064453125\n",
            "EPOCHS: 87/400---------Loss: 978.9196166992188\n",
            "EPOCHS: 88/400---------Loss: 975.7716674804688\n",
            "EPOCHS: 89/400---------Loss: 972.6400146484375\n",
            "EPOCHS: 90/400---------Loss: 969.5245971679688\n",
            "EPOCHS: 91/400---------Loss: 966.4254150390625\n",
            "EPOCHS: 92/400---------Loss: 963.3419189453125\n",
            "EPOCHS: 93/400---------Loss: 960.2745971679688\n",
            "EPOCHS: 94/400---------Loss: 957.2232666015625\n",
            "EPOCHS: 95/400---------Loss: 954.1873168945312\n",
            "EPOCHS: 96/400---------Loss: 951.1672973632812\n",
            "EPOCHS: 97/400---------Loss: 948.1629028320312\n",
            "EPOCHS: 98/400---------Loss: 945.1736450195312\n",
            "EPOCHS: 99/400---------Loss: 942.2001953125\n",
            "EPOCHS: 100/400---------Loss: 939.2418823242188\n",
            "EPOCHS: 101/400---------Loss: 936.298828125\n",
            "EPOCHS: 102/400---------Loss: 933.3707885742188\n",
            "EPOCHS: 103/400---------Loss: 930.4580078125\n",
            "EPOCHS: 104/400---------Loss: 927.5598754882812\n",
            "EPOCHS: 105/400---------Loss: 924.6769409179688\n",
            "EPOCHS: 106/400---------Loss: 921.8084716796875\n",
            "EPOCHS: 107/400---------Loss: 918.955078125\n",
            "EPOCHS: 108/400---------Loss: 916.1160888671875\n",
            "EPOCHS: 109/400---------Loss: 913.2913818359375\n",
            "EPOCHS: 110/400---------Loss: 910.4811401367188\n",
            "EPOCHS: 111/400---------Loss: 907.6856689453125\n",
            "EPOCHS: 112/400---------Loss: 904.9041137695312\n",
            "EPOCHS: 113/400---------Loss: 902.1370849609375\n",
            "EPOCHS: 114/400---------Loss: 899.3839721679688\n",
            "EPOCHS: 115/400---------Loss: 896.6447143554688\n",
            "EPOCHS: 116/400---------Loss: 893.9196166992188\n",
            "EPOCHS: 117/400---------Loss: 891.2081909179688\n",
            "EPOCHS: 118/400---------Loss: 888.5110473632812\n",
            "EPOCHS: 119/400---------Loss: 885.8270263671875\n",
            "EPOCHS: 120/400---------Loss: 883.1568603515625\n",
            "EPOCHS: 121/400---------Loss: 880.5003662109375\n",
            "EPOCHS: 122/400---------Loss: 877.8571166992188\n",
            "EPOCHS: 123/400---------Loss: 875.2275390625\n",
            "EPOCHS: 124/400---------Loss: 872.6110229492188\n",
            "EPOCHS: 125/400---------Loss: 870.0079345703125\n",
            "EPOCHS: 126/400---------Loss: 867.41796875\n",
            "EPOCHS: 127/400---------Loss: 864.8410034179688\n",
            "EPOCHS: 128/400---------Loss: 862.2769775390625\n",
            "EPOCHS: 129/400---------Loss: 859.7259521484375\n",
            "EPOCHS: 130/400---------Loss: 857.1878662109375\n",
            "EPOCHS: 131/400---------Loss: 854.6627197265625\n",
            "EPOCHS: 132/400---------Loss: 852.1500854492188\n",
            "EPOCHS: 133/400---------Loss: 849.6500244140625\n",
            "EPOCHS: 134/400---------Loss: 847.1624755859375\n",
            "EPOCHS: 135/400---------Loss: 844.6876831054688\n",
            "EPOCHS: 136/400---------Loss: 842.2252807617188\n",
            "EPOCHS: 137/400---------Loss: 839.7752685546875\n",
            "EPOCHS: 138/400---------Loss: 837.33740234375\n",
            "EPOCHS: 139/400---------Loss: 834.9117431640625\n",
            "EPOCHS: 140/400---------Loss: 832.498046875\n",
            "EPOCHS: 141/400---------Loss: 830.0968627929688\n",
            "EPOCHS: 142/400---------Loss: 827.7072143554688\n",
            "EPOCHS: 143/400---------Loss: 825.3299560546875\n",
            "EPOCHS: 144/400---------Loss: 822.96435546875\n",
            "EPOCHS: 145/400---------Loss: 820.6104736328125\n",
            "EPOCHS: 146/400---------Loss: 818.2686767578125\n",
            "EPOCHS: 147/400---------Loss: 815.9381103515625\n",
            "EPOCHS: 148/400---------Loss: 813.619140625\n",
            "EPOCHS: 149/400---------Loss: 811.31201171875\n",
            "EPOCHS: 150/400---------Loss: 809.0162963867188\n",
            "EPOCHS: 151/400---------Loss: 806.7320556640625\n",
            "EPOCHS: 152/400---------Loss: 804.4588012695312\n",
            "EPOCHS: 153/400---------Loss: 802.1968994140625\n",
            "EPOCHS: 154/400---------Loss: 799.9464721679688\n",
            "EPOCHS: 155/400---------Loss: 797.70703125\n",
            "EPOCHS: 156/400---------Loss: 795.4784545898438\n",
            "EPOCHS: 157/400---------Loss: 793.2611694335938\n",
            "EPOCHS: 158/400---------Loss: 791.0546875\n",
            "EPOCHS: 159/400---------Loss: 788.859375\n",
            "EPOCHS: 160/400---------Loss: 786.6746826171875\n",
            "EPOCHS: 161/400---------Loss: 784.5007934570312\n",
            "EPOCHS: 162/400---------Loss: 782.3377075195312\n",
            "EPOCHS: 163/400---------Loss: 780.1849365234375\n",
            "EPOCHS: 164/400---------Loss: 778.0430297851562\n",
            "EPOCHS: 165/400---------Loss: 775.9115600585938\n",
            "EPOCHS: 166/400---------Loss: 773.79052734375\n",
            "EPOCHS: 167/400---------Loss: 771.6798095703125\n",
            "EPOCHS: 168/400---------Loss: 769.5797119140625\n",
            "EPOCHS: 169/400---------Loss: 767.4896850585938\n",
            "EPOCHS: 170/400---------Loss: 765.4100341796875\n",
            "EPOCHS: 171/400---------Loss: 763.3405151367188\n",
            "EPOCHS: 172/400---------Loss: 761.2809448242188\n",
            "EPOCHS: 173/400---------Loss: 759.231689453125\n",
            "EPOCHS: 174/400---------Loss: 757.1922607421875\n",
            "EPOCHS: 175/400---------Loss: 755.1627807617188\n",
            "EPOCHS: 176/400---------Loss: 753.14306640625\n",
            "EPOCHS: 177/400---------Loss: 751.1334838867188\n",
            "EPOCHS: 178/400---------Loss: 749.1334228515625\n",
            "EPOCHS: 179/400---------Loss: 747.1432495117188\n",
            "EPOCHS: 180/400---------Loss: 745.16259765625\n",
            "EPOCHS: 181/400---------Loss: 743.1917724609375\n",
            "EPOCHS: 182/400---------Loss: 741.230224609375\n",
            "EPOCHS: 183/400---------Loss: 739.2781982421875\n",
            "EPOCHS: 184/400---------Loss: 737.335693359375\n",
            "EPOCHS: 185/400---------Loss: 735.4026489257812\n",
            "EPOCHS: 186/400---------Loss: 733.4788818359375\n",
            "EPOCHS: 187/400---------Loss: 731.5643310546875\n",
            "EPOCHS: 188/400---------Loss: 729.6590576171875\n",
            "EPOCHS: 189/400---------Loss: 727.7633056640625\n",
            "EPOCHS: 190/400---------Loss: 725.8762817382812\n",
            "EPOCHS: 191/400---------Loss: 723.9983520507812\n",
            "EPOCHS: 192/400---------Loss: 722.1295166015625\n",
            "EPOCHS: 193/400---------Loss: 720.269775390625\n",
            "EPOCHS: 194/400---------Loss: 718.4188232421875\n",
            "EPOCHS: 195/400---------Loss: 716.5767822265625\n",
            "EPOCHS: 196/400---------Loss: 714.7435302734375\n",
            "EPOCHS: 197/400---------Loss: 712.9190673828125\n",
            "EPOCHS: 198/400---------Loss: 711.1034545898438\n",
            "EPOCHS: 199/400---------Loss: 709.2965087890625\n",
            "EPOCHS: 200/400---------Loss: 707.4979248046875\n",
            "EPOCHS: 201/400---------Loss: 705.7081298828125\n",
            "EPOCHS: 202/400---------Loss: 703.9268188476562\n",
            "EPOCHS: 203/400---------Loss: 702.1541137695312\n",
            "EPOCHS: 204/400---------Loss: 700.3897094726562\n",
            "EPOCHS: 205/400---------Loss: 698.6336669921875\n",
            "EPOCHS: 206/400---------Loss: 696.8860473632812\n",
            "EPOCHS: 207/400---------Loss: 695.1466674804688\n",
            "EPOCHS: 208/400---------Loss: 693.41552734375\n",
            "EPOCHS: 209/400---------Loss: 691.6927490234375\n",
            "EPOCHS: 210/400---------Loss: 689.97802734375\n",
            "EPOCHS: 211/400---------Loss: 688.2714233398438\n",
            "EPOCHS: 212/400---------Loss: 686.57275390625\n",
            "EPOCHS: 213/400---------Loss: 684.8824462890625\n",
            "EPOCHS: 214/400---------Loss: 683.1998291015625\n",
            "EPOCHS: 215/400---------Loss: 681.5252685546875\n",
            "EPOCHS: 216/400---------Loss: 679.8582763671875\n",
            "EPOCHS: 217/400---------Loss: 678.1995849609375\n",
            "EPOCHS: 218/400---------Loss: 676.5484008789062\n",
            "EPOCHS: 219/400---------Loss: 674.9052734375\n",
            "EPOCHS: 220/400---------Loss: 673.26953125\n",
            "EPOCHS: 221/400---------Loss: 671.6414794921875\n",
            "EPOCHS: 222/400---------Loss: 670.0213623046875\n",
            "EPOCHS: 223/400---------Loss: 668.4085693359375\n",
            "EPOCHS: 224/400---------Loss: 666.8033447265625\n",
            "EPOCHS: 225/400---------Loss: 665.2057495117188\n",
            "EPOCHS: 226/400---------Loss: 663.6153564453125\n",
            "EPOCHS: 227/400---------Loss: 662.0325927734375\n",
            "EPOCHS: 228/400---------Loss: 660.4570922851562\n",
            "EPOCHS: 229/400---------Loss: 658.8888549804688\n",
            "EPOCHS: 230/400---------Loss: 657.3282470703125\n",
            "EPOCHS: 231/400---------Loss: 655.7747192382812\n",
            "EPOCHS: 232/400---------Loss: 654.2281494140625\n",
            "EPOCHS: 233/400---------Loss: 652.6890869140625\n",
            "EPOCHS: 234/400---------Loss: 651.1571044921875\n",
            "EPOCHS: 235/400---------Loss: 649.6322021484375\n",
            "EPOCHS: 236/400---------Loss: 648.1142578125\n",
            "EPOCHS: 237/400---------Loss: 646.6033325195312\n",
            "EPOCHS: 238/400---------Loss: 645.0995483398438\n",
            "EPOCHS: 239/400---------Loss: 643.6024169921875\n",
            "EPOCHS: 240/400---------Loss: 642.1126098632812\n",
            "EPOCHS: 241/400---------Loss: 640.6292724609375\n",
            "EPOCHS: 242/400---------Loss: 639.15283203125\n",
            "EPOCHS: 243/400---------Loss: 637.6831665039062\n",
            "EPOCHS: 244/400---------Loss: 636.2203369140625\n",
            "EPOCHS: 245/400---------Loss: 634.7643432617188\n",
            "EPOCHS: 246/400---------Loss: 633.3150024414062\n",
            "EPOCHS: 247/400---------Loss: 631.8721923828125\n",
            "EPOCHS: 248/400---------Loss: 630.4358520507812\n",
            "EPOCHS: 249/400---------Loss: 629.0062866210938\n",
            "EPOCHS: 250/400---------Loss: 627.5830078125\n",
            "EPOCHS: 251/400---------Loss: 626.1663208007812\n",
            "EPOCHS: 252/400---------Loss: 624.7562255859375\n",
            "EPOCHS: 253/400---------Loss: 623.3524169921875\n",
            "EPOCHS: 254/400---------Loss: 621.9552001953125\n",
            "EPOCHS: 255/400---------Loss: 620.5640869140625\n",
            "EPOCHS: 256/400---------Loss: 619.179443359375\n",
            "EPOCHS: 257/400---------Loss: 617.8009033203125\n",
            "EPOCHS: 258/400---------Loss: 616.4290771484375\n",
            "EPOCHS: 259/400---------Loss: 615.06298828125\n",
            "EPOCHS: 260/400---------Loss: 613.7032470703125\n",
            "EPOCHS: 261/400---------Loss: 612.3494873046875\n",
            "EPOCHS: 262/400---------Loss: 611.0018920898438\n",
            "EPOCHS: 263/400---------Loss: 609.6607666015625\n",
            "EPOCHS: 264/400---------Loss: 608.3253173828125\n",
            "EPOCHS: 265/400---------Loss: 606.99609375\n",
            "EPOCHS: 266/400---------Loss: 605.6727294921875\n",
            "EPOCHS: 267/400---------Loss: 604.35498046875\n",
            "EPOCHS: 268/400---------Loss: 603.04345703125\n",
            "EPOCHS: 269/400---------Loss: 601.73779296875\n",
            "EPOCHS: 270/400---------Loss: 600.4381103515625\n",
            "EPOCHS: 271/400---------Loss: 599.1441040039062\n",
            "EPOCHS: 272/400---------Loss: 597.8558349609375\n",
            "EPOCHS: 273/400---------Loss: 596.5735473632812\n",
            "EPOCHS: 274/400---------Loss: 595.296630859375\n",
            "EPOCHS: 275/400---------Loss: 594.0256958007812\n",
            "EPOCHS: 276/400---------Loss: 592.7603759765625\n",
            "EPOCHS: 277/400---------Loss: 591.5006103515625\n",
            "EPOCHS: 278/400---------Loss: 590.2464599609375\n",
            "EPOCHS: 279/400---------Loss: 588.9979858398438\n",
            "EPOCHS: 280/400---------Loss: 587.7550048828125\n",
            "EPOCHS: 281/400---------Loss: 586.5174560546875\n",
            "EPOCHS: 282/400---------Loss: 585.2853393554688\n",
            "EPOCHS: 283/400---------Loss: 584.0587158203125\n",
            "EPOCHS: 284/400---------Loss: 582.8377075195312\n",
            "EPOCHS: 285/400---------Loss: 581.621826171875\n",
            "EPOCHS: 286/400---------Loss: 580.4114379882812\n",
            "EPOCHS: 287/400---------Loss: 579.2064208984375\n",
            "EPOCHS: 288/400---------Loss: 578.0064697265625\n",
            "EPOCHS: 289/400---------Loss: 576.8120727539062\n",
            "EPOCHS: 290/400---------Loss: 575.6227416992188\n",
            "EPOCHS: 291/400---------Loss: 574.4387817382812\n",
            "EPOCHS: 292/400---------Loss: 573.2598876953125\n",
            "EPOCHS: 293/400---------Loss: 572.086181640625\n",
            "EPOCHS: 294/400---------Loss: 570.9176635742188\n",
            "EPOCHS: 295/400---------Loss: 569.754150390625\n",
            "EPOCHS: 296/400---------Loss: 568.5960083007812\n",
            "EPOCHS: 297/400---------Loss: 567.4424438476562\n",
            "EPOCHS: 298/400---------Loss: 566.2940673828125\n",
            "EPOCHS: 299/400---------Loss: 565.1509399414062\n",
            "EPOCHS: 300/400---------Loss: 564.0126342773438\n",
            "EPOCHS: 301/400---------Loss: 562.8792114257812\n",
            "EPOCHS: 302/400---------Loss: 561.750732421875\n",
            "EPOCHS: 303/400---------Loss: 560.6271362304688\n",
            "EPOCHS: 304/400---------Loss: 559.508544921875\n",
            "EPOCHS: 305/400---------Loss: 558.3946533203125\n",
            "EPOCHS: 306/400---------Loss: 557.2857666015625\n",
            "EPOCHS: 307/400---------Loss: 556.1812744140625\n",
            "EPOCHS: 308/400---------Loss: 555.0816650390625\n",
            "EPOCHS: 309/400---------Loss: 553.987060546875\n",
            "EPOCHS: 310/400---------Loss: 552.89697265625\n",
            "EPOCHS: 311/400---------Loss: 551.8116455078125\n",
            "EPOCHS: 312/400---------Loss: 550.7308959960938\n",
            "EPOCHS: 313/400---------Loss: 549.65478515625\n",
            "EPOCHS: 314/400---------Loss: 548.5834350585938\n",
            "EPOCHS: 315/400---------Loss: 547.5164794921875\n",
            "EPOCHS: 316/400---------Loss: 546.4542236328125\n",
            "EPOCHS: 317/400---------Loss: 545.3963623046875\n",
            "EPOCHS: 318/400---------Loss: 544.3430786132812\n",
            "EPOCHS: 319/400---------Loss: 543.2943725585938\n",
            "EPOCHS: 320/400---------Loss: 542.2501220703125\n",
            "EPOCHS: 321/400---------Loss: 541.210205078125\n",
            "EPOCHS: 322/400---------Loss: 540.1747436523438\n",
            "EPOCHS: 323/400---------Loss: 539.1436767578125\n",
            "EPOCHS: 324/400---------Loss: 538.1170043945312\n",
            "EPOCHS: 325/400---------Loss: 537.0948486328125\n",
            "EPOCHS: 326/400---------Loss: 536.0767211914062\n",
            "EPOCHS: 327/400---------Loss: 535.0631103515625\n",
            "EPOCHS: 328/400---------Loss: 534.0536499023438\n",
            "EPOCHS: 329/400---------Loss: 533.0484619140625\n",
            "EPOCHS: 330/400---------Loss: 532.0475463867188\n",
            "EPOCHS: 331/400---------Loss: 531.05078125\n",
            "EPOCHS: 332/400---------Loss: 530.0582885742188\n",
            "EPOCHS: 333/400---------Loss: 529.0699462890625\n",
            "EPOCHS: 334/400---------Loss: 528.085693359375\n",
            "EPOCHS: 335/400---------Loss: 527.1056518554688\n",
            "EPOCHS: 336/400---------Loss: 526.1295166015625\n",
            "EPOCHS: 337/400---------Loss: 525.1575927734375\n",
            "EPOCHS: 338/400---------Loss: 524.18994140625\n",
            "EPOCHS: 339/400---------Loss: 523.22607421875\n",
            "EPOCHS: 340/400---------Loss: 522.2662963867188\n",
            "EPOCHS: 341/400---------Loss: 521.3104858398438\n",
            "EPOCHS: 342/400---------Loss: 520.3587646484375\n",
            "EPOCHS: 343/400---------Loss: 519.4108276367188\n",
            "EPOCHS: 344/400---------Loss: 518.4669799804688\n",
            "EPOCHS: 345/400---------Loss: 517.5269165039062\n",
            "EPOCHS: 346/400---------Loss: 516.5908813476562\n",
            "EPOCHS: 347/400---------Loss: 515.6585693359375\n",
            "EPOCHS: 348/400---------Loss: 514.7301025390625\n",
            "EPOCHS: 349/400---------Loss: 513.8055419921875\n",
            "EPOCHS: 350/400---------Loss: 512.8847045898438\n",
            "EPOCHS: 351/400---------Loss: 511.9677734375\n",
            "EPOCHS: 352/400---------Loss: 511.05462646484375\n",
            "EPOCHS: 353/400---------Loss: 510.14520263671875\n",
            "EPOCHS: 354/400---------Loss: 509.23956298828125\n",
            "EPOCHS: 355/400---------Loss: 508.33746337890625\n",
            "EPOCHS: 356/400---------Loss: 507.4393005371094\n",
            "EPOCHS: 357/400---------Loss: 506.5445861816406\n",
            "EPOCHS: 358/400---------Loss: 505.6536560058594\n",
            "EPOCHS: 359/400---------Loss: 504.76629638671875\n",
            "EPOCHS: 360/400---------Loss: 503.88262939453125\n",
            "EPOCHS: 361/400---------Loss: 503.00250244140625\n",
            "EPOCHS: 362/400---------Loss: 502.1259765625\n",
            "EPOCHS: 363/400---------Loss: 501.2530212402344\n",
            "EPOCHS: 364/400---------Loss: 500.38360595703125\n",
            "EPOCHS: 365/400---------Loss: 499.5176696777344\n",
            "EPOCHS: 366/400---------Loss: 498.6552734375\n",
            "EPOCHS: 367/400---------Loss: 497.7964782714844\n",
            "EPOCHS: 368/400---------Loss: 496.94091796875\n",
            "EPOCHS: 369/400---------Loss: 496.08892822265625\n",
            "EPOCHS: 370/400---------Loss: 495.2403259277344\n",
            "EPOCHS: 371/400---------Loss: 494.39532470703125\n",
            "EPOCHS: 372/400---------Loss: 493.55352783203125\n",
            "EPOCHS: 373/400---------Loss: 492.7152404785156\n",
            "EPOCHS: 374/400---------Loss: 491.88018798828125\n",
            "EPOCHS: 375/400---------Loss: 491.0484924316406\n",
            "EPOCHS: 376/400---------Loss: 490.22021484375\n",
            "EPOCHS: 377/400---------Loss: 489.39532470703125\n",
            "EPOCHS: 378/400---------Loss: 488.57354736328125\n",
            "EPOCHS: 379/400---------Loss: 487.755126953125\n",
            "EPOCHS: 380/400---------Loss: 486.94000244140625\n",
            "EPOCHS: 381/400---------Loss: 486.1280212402344\n",
            "EPOCHS: 382/400---------Loss: 485.3193359375\n",
            "EPOCHS: 383/400---------Loss: 484.51397705078125\n",
            "EPOCHS: 384/400---------Loss: 483.71173095703125\n",
            "EPOCHS: 385/400---------Loss: 482.91259765625\n",
            "EPOCHS: 386/400---------Loss: 482.1166076660156\n",
            "EPOCHS: 387/400---------Loss: 481.32391357421875\n",
            "EPOCHS: 388/400---------Loss: 480.5341796875\n",
            "EPOCHS: 389/400---------Loss: 479.74755859375\n",
            "EPOCHS: 390/400---------Loss: 478.96429443359375\n",
            "EPOCHS: 391/400---------Loss: 478.18389892578125\n",
            "EPOCHS: 392/400---------Loss: 477.40667724609375\n",
            "EPOCHS: 393/400---------Loss: 476.6324157714844\n",
            "EPOCHS: 394/400---------Loss: 475.86138916015625\n",
            "EPOCHS: 395/400---------Loss: 475.09307861328125\n",
            "EPOCHS: 396/400---------Loss: 474.3280334472656\n",
            "EPOCHS: 397/400---------Loss: 473.5658264160156\n",
            "EPOCHS: 398/400---------Loss: 472.8065490722656\n",
            "EPOCHS: 399/400---------Loss: 472.0504455566406\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preds = model(inputs)\n",
        "preds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cKWxdlNCg4a_",
        "outputId": "9720987e-8138-426f-edc9-8e6d5ff1ee25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[53.1036, 68.9289],\n",
              "        [49.1375, 42.1784],\n",
              "        [80.1249, 75.3134],\n",
              "        [40.9827, 61.0539],\n",
              "        [93.6729, 70.3700]], grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "targets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3hFfA9OhXUk",
        "outputId": "ad1dd174-10c8-4305-a269-dc0a89ad07ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 32.,  66.],\n",
              "        [ 52.,  76.],\n",
              "        [102.,  55.],\n",
              "        [ 32.,  26.],\n",
              "        [ 99., 100.]])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using Torch built in function\n"
      ],
      "metadata": {
        "id": "hb4mZZJE3b0N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using Torch built in function\n",
        "import torch.nn as nn\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "HDQ7zyzdhaKL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Input (temp, rainfall, humidity)\n",
        "inputs = np.array([[73, 67, 43],\n",
        "                   [91, 88, 64],\n",
        "                   [87, 134, 58],\n",
        "                   [102, 43, 37],\n",
        "                   [69, 96, 70],\n",
        "                   [74, 66, 43],\n",
        "                   [91, 87, 65],\n",
        "                   [88, 134, 59],\n",
        "                   [101, 44, 37],\n",
        "                   [68, 96, 71],\n",
        "                   [73, 66, 44],\n",
        "                   [92, 87, 64],\n",
        "                   [87, 135, 57],\n",
        "                   [103, 43, 36],\n",
        "                   [68, 97, 70]],\n",
        "                  dtype='float32')\n",
        "\n",
        "# Targets (apples, oranges)\n",
        "targets = np.array([[56, 70],\n",
        "                    [81, 101],\n",
        "                    [119, 133],\n",
        "                    [22, 37],\n",
        "                    [103, 119],\n",
        "                    [57, 69],\n",
        "                    [80, 102],\n",
        "                    [118, 132],\n",
        "                    [21, 38],\n",
        "                    [104, 118],\n",
        "                    [57, 69],\n",
        "                    [82, 100],\n",
        "                    [118, 134],\n",
        "                    [20, 38],\n",
        "                    [102, 120]],\n",
        "                   dtype='float32')\n",
        "\n",
        "inputs = torch.from_numpy(inputs)\n",
        "targets = torch.from_numpy(targets)\n"
      ],
      "metadata": {
        "id": "Hk2NE2Hb32An"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lgmbZi0U6N1W",
        "outputId": "78c3f7bd-067d-4128-82b9-347d47d74913"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 73.,  67.,  43.],\n",
              "        [ 91.,  88.,  64.],\n",
              "        [ 87., 134.,  58.],\n",
              "        [102.,  43.,  37.],\n",
              "        [ 69.,  96.,  70.],\n",
              "        [ 74.,  66.,  43.],\n",
              "        [ 91.,  87.,  65.],\n",
              "        [ 88., 134.,  59.],\n",
              "        [101.,  44.,  37.],\n",
              "        [ 68.,  96.,  71.],\n",
              "        [ 73.,  66.,  44.],\n",
              "        [ 92.,  87.,  64.],\n",
              "        [ 87., 135.,  57.],\n",
              "        [103.,  43.,  36.],\n",
              "        [ 68.,  97.,  70.]])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are using 15 training examples to illustrate how to work with large datasets in small batches.\n",
        "\n",
        "# Dataset and DataLoader\n",
        "We'll create a TensorDataset, which allows access to rows from inputs and targets as tuples, and provides standard APIs for working with many different types of datasets in PyTorch."
      ],
      "metadata": {
        "id": "z6s1U7kak5fN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset"
      ],
      "metadata": {
        "id": "794jCXqE6eRh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define dataset\n",
        "train_ds = TensorDataset(inputs, targets)\n",
        "train_ds[0:3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IorETMFUltNi",
        "outputId": "aef60ff1-dea3-45a4-cafc-6398d886a7f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[ 73.,  67.,  43.],\n",
              "         [ 91.,  88.,  64.],\n",
              "         [ 87., 134.,  58.]]),\n",
              " tensor([[ 56.,  70.],\n",
              "         [ 81., 101.],\n",
              "         [119., 133.]]))"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The TensorDataset allows us to access a small section of the training data using the array indexing notation ([0:3] in the above code). It returns a tuple with two elements. The first element contains the input variables for the selected rows, and the second contains the targets.\n",
        "\n",
        "We'll also create a DataLoader, which can split the data into batches of a predefined size while training. It also provides other utilities like shuffling and random sampling of the data."
      ],
      "metadata": {
        "id": "EDvdJaX3mX3G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader"
      ],
      "metadata": {
        "id": "agevv9i5lxFF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define data loader\n",
        "batch_size = 5\n",
        "train_dl = DataLoader(train_ds, batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "ifupzZD0n2qc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use the data loader in a for loop. Let's look at an example."
      ],
      "metadata": {
        "id": "eKy602VVoZCT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for xb, yb in train_dl:\n",
        "    print(xb)\n",
        "    print(yb)\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H5TmgGOPn_sw",
        "outputId": "3c81bfe8-680a-4bd7-9ef1-c623821c681d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 87., 135.,  57.],\n",
            "        [ 73.,  67.,  43.],\n",
            "        [ 69.,  96.,  70.],\n",
            "        [ 73.,  66.,  44.],\n",
            "        [ 91.,  87.,  65.]])\n",
            "tensor([[118., 134.],\n",
            "        [ 56.,  70.],\n",
            "        [103., 119.],\n",
            "        [ 57.,  69.],\n",
            "        [ 80., 102.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In each iteration, the data loader returns one batch of data with the given batch size. If shuffle is set to True, it shuffles the training data before creating batches. Shuffling helps randomize the input to the optimization algorithm, leading to a faster reduction in the loss.\n",
        "\n",
        "#nn.Linear\n",
        "Instead of initializing the weights & biases manually, we can define the model using the nn.Linear class from PyTorch, which does it automatically."
      ],
      "metadata": {
        "id": "1Ig6owi4tldJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#nn.Linear\n",
        "# Define model\n",
        "model = nn.Linear(3, 2) # 3 features 2 outputs\n",
        "print(model.weight)\n",
        "print(model.bias)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tq3U1Y-qoc-z",
        "outputId": "79db4f41-d9a8-490f-bbdc-3be8354c4c83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[-0.0244,  0.1924, -0.4866],\n",
            "        [ 0.4739, -0.2806, -0.1876]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([ 0.1623, -0.1380], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PyTorch models also have a helpful .parameters method, which returns a list containing all the weights and bias matrices present in the model. For our linear regression model, we have one weight matrix and one bias matrix."
      ],
      "metadata": {
        "id": "XB_yb8D_vBAi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "list(model.parameters())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cXYDOwgMtujg",
        "outputId": "aeada2a3-b4e2-4c08-b7b9-d902f96a1653"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Parameter containing:\n",
              " tensor([[-0.0244,  0.1924, -0.4866],\n",
              "         [ 0.4739, -0.2806, -0.1876]], requires_grad=True),\n",
              " Parameter containing:\n",
              " tensor([ 0.1623, -0.1380], requires_grad=True)]"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate predictions\n",
        "preds = model(inputs)\n",
        "preds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AH-wAXNsvEzM",
        "outputId": "fa58c14f-3a1c-4059-bf13-08f8862f5508"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ -9.6546,   7.5915],\n",
              "        [-16.2731,   6.2901],\n",
              "        [ -4.4067,  -7.3883],\n",
              "        [-12.0591,  29.1954],\n",
              "        [-17.1173,  -7.5065],\n",
              "        [ -9.8714,   8.3460],\n",
              "        [-16.9522,   6.3832],\n",
              "        [ -4.9177,  -7.1020],\n",
              "        [-11.8424,  28.4408],\n",
              "        [-17.5796,  -8.1679],\n",
              "        [-10.3336,   7.6845],\n",
              "        [-16.4899,   7.0447],\n",
              "        [ -3.7277,  -7.4814],\n",
              "        [-11.5969,  29.8569],\n",
              "        [-16.9006,  -8.2610]], grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loss Function\n",
        "Instead of defining a loss function manually, we can use the built-in loss function mse_loss."
      ],
      "metadata": {
        "id": "HopFz9iFwi9P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "2EEdc3QWvPh_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The nn.functional package contains many useful loss functions and several other utilities."
      ],
      "metadata": {
        "id": "K5_SdmdswpIO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define loss function\n",
        "loss_fn = F.mse_loss"
      ],
      "metadata": {
        "id": "JeqHTlCIwpwE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss = loss_fn(model(inputs), targets)\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-fgQLxzlw_Hy",
        "outputId": "54339689-e773-4f14-c45c-626d449d83fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(9312.3789, grad_fn=<MseLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Optimizer\n",
        "Instead of manually manipulating the model's weights & biases using gradients, we can use the optimizer optim.SGD. SGD is short for \"stochastic gradient descent\". The term stochastic indicates that samples are selected in random batches instead of as a single group."
      ],
      "metadata": {
        "id": "i4Bc_7jqxZRa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# optimizer\n",
        "opt = torch.optim.SGD(model.parameters(), lr = 1e-5)"
      ],
      "metadata": {
        "id": "TvBzKAchxJls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VKa_B_zn854Y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}