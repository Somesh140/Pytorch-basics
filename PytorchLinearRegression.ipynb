{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMb1VwrvT3ug9V6Xu2hcUpw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Somesh140/Pytorch-basics/blob/main/PytorchLinearRegression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "SASHo_ovbRH6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb1d4e65-2f3d-4f53-e983-14652ec86d78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Jun 21 05:26:04 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   56C    P8    10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch"
      ],
      "metadata": {
        "id": "EutFA8QbiWH0"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Input data\n",
        "input = np.array([[32,55,77],\n",
        "                  [31,75,57],\n",
        "                  [52,55,77],\n",
        "                  [22,100,87],\n",
        "                  [62,80,77]], dtype = 'float32' )\n",
        "input"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8Z-0vckryVd",
        "outputId": "1e28d270-3c1c-47fb-90a5-b4d5e579feec"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 32.,  55.,  77.],\n",
              "       [ 31.,  75.,  57.],\n",
              "       [ 52.,  55.,  77.],\n",
              "       [ 22., 100.,  87.],\n",
              "       [ 62.,  80.,  77.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# target\n",
        "target = np.array([[32,66],\n",
        "                   [52,76],\n",
        "                   [102,55],\n",
        "                   [32,26],\n",
        "                   [99,100]\n",
        "                   ],dtype='float32')\n",
        "target"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-h_GiBcCr_xe",
        "outputId": "b80b3746-bfe8-4d1b-d469-1915ca15a7cf"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 32.,  66.],\n",
              "       [ 52.,  76.],\n",
              "       [102.,  55.],\n",
              "       [ 32.,  26.],\n",
              "       [ 99., 100.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = torch.from_numpy(input)\n",
        "targets = torch.from_numpy(target)\n",
        "print(inputs,targets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w9FjJMmSTz7F",
        "outputId": "eb0bf5fd-fda2-4f34-900e-3bc6177d46e7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 32.,  55.,  77.],\n",
            "        [ 31.,  75.,  57.],\n",
            "        [ 52.,  55.,  77.],\n",
            "        [ 22., 100.,  87.],\n",
            "        [ 62.,  80.,  77.]]) tensor([[ 32.,  66.],\n",
            "        [ 52.,  76.],\n",
            "        [102.,  55.],\n",
            "        [ 32.,  26.],\n",
            "        [ 99., 100.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# weights and biases\n",
        "w = torch.randn(2,3, requires_grad = True) # 2 output column and 3 input columns\n",
        "b = torch.randn(2, requires_grad= True) # 2 output columns\n",
        "\n",
        "print(w)\n",
        "print(b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZuR8IbduXSrZ",
        "outputId": "7b598ba1-0a83-4c3b-e673-b2369185f06e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 1.8602,  0.2943, -0.9972],\n",
            "        [-0.3848,  0.2942,  1.5551]], requires_grad=True)\n",
            "tensor([-1.2363,  1.2772], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def model(x):\n",
        "  return x@w.t() + b"
      ],
      "metadata": {
        "id": "cKbTYvh9YVz8"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds = model(inputs)\n",
        "print(preds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qTrl4Zbi0VQm",
        "outputId": "8987895c-aed1-4fd9-8d5b-b7aab7793e2a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ -2.3052, 124.8861],\n",
            "        [ 21.6648, 100.0537],\n",
            "        [ 34.8984, 117.1894],\n",
            "        [-17.6340, 157.5252],\n",
            "        [ 60.8585, 120.6966]], grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "targets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jOXx8rjE0sqy",
        "outputId": "eda2de09-a744-499e-c3a0-0a0765cbf3fc"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 32.,  66.],\n",
              "        [ 52.,  76.],\n",
              "        [102.,  55.],\n",
              "        [ 32.,  26.],\n",
              "        [ 99., 100.]])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# loss\n",
        "def MSE(t1,t2):\n",
        "  diff = t1 -t2\n",
        "  return torch.sum(diff*diff)/diff.numel()"
      ],
      "metadata": {
        "id": "JZIniFRV0vlE"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compute loss\n",
        "loss = MSE(preds, targets)\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eocpElPv1qay",
        "outputId": "1910bbc3-2d92-4e39-b04c-34eea2f4c858"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(3615.8906, grad_fn=<DivBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute gradient\n",
        "loss.backward()"
      ],
      "metadata": {
        "id": "lnSUxoB710Lt"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(w)\n",
        "print(w.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E2FPvnyU3ujc",
        "outputId": "9dc128ec-9f74-4629-9232-f9750181e40b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 1.8602,  0.2943, -0.9972],\n",
            "        [-0.3848,  0.2942,  1.5551]], requires_grad=True)\n",
            "tensor([[-1796.8325, -3173.4463, -3358.4963],\n",
            "        [ 2008.1223,  4654.2852,  4746.0410]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad(): # no_grad to disable gradient calculation\n",
        "  w -= w.grad * 1e-5\n",
        "  b -= b.grad * 1e-5"
      ],
      "metadata": {
        "id": "1aenW-tY30E5"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(w)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GiYsHp175u5y",
        "outputId": "00faa2f4-32fc-4f21-8a0d-07c1ec46f28f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 1.8781,  0.3261, -0.9636],\n",
            "        [-0.4049,  0.2477,  1.5076]], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preds = model(inputs)"
      ],
      "metadata": {
        "id": "K4bUw52C6NTJ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss = MSE(preds, targets)\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SyMVOGV57gMp",
        "outputId": "cd7206b3-1960-475a-b0d7-1976e169a16d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(2934.0601, grad_fn=<DivBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training\n",
        "n = 400\n",
        "for i in range(n):\n",
        "  preds = model(inputs)\n",
        "  loss = MSE(preds, targets)\n",
        "  loss.backward()\n",
        "  with torch.no_grad(): # no_grad to disable gradient calculation\n",
        "    w -= w.grad * 1e-5\n",
        "    b -= b.grad * 1e-5\n",
        "\n",
        "  w.grad.zero_() # to clear the previous gradient values\n",
        "  b.grad.zero_()\n",
        "\n",
        "  print(f\"EPOCHS: {i}/{n}---------Loss: {loss}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D_KtTqgC7jCv",
        "outputId": "c67ea3e7-28ec-4883-995c-431d53df7cc7"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCHS: 0/400---------Loss: 2934.06005859375\n",
            "EPOCHS: 1/400---------Loss: 1905.5367431640625\n",
            "EPOCHS: 2/400---------Loss: 1629.979736328125\n",
            "EPOCHS: 3/400---------Loss: 1419.6165771484375\n",
            "EPOCHS: 4/400---------Loss: 1258.905517578125\n",
            "EPOCHS: 5/400---------Loss: 1136.0096435546875\n",
            "EPOCHS: 6/400---------Loss: 1041.9146728515625\n",
            "EPOCHS: 7/400---------Loss: 969.7557373046875\n",
            "EPOCHS: 8/400---------Loss: 914.3048706054688\n",
            "EPOCHS: 9/400---------Loss: 871.5807495117188\n",
            "EPOCHS: 10/400---------Loss: 838.5510864257812\n",
            "EPOCHS: 11/400---------Loss: 812.9070434570312\n",
            "EPOCHS: 12/400---------Loss: 792.8897705078125\n",
            "EPOCHS: 13/400---------Loss: 777.1604614257812\n",
            "EPOCHS: 14/400---------Loss: 764.6987915039062\n",
            "EPOCHS: 15/400---------Loss: 754.7279052734375\n",
            "EPOCHS: 16/400---------Loss: 746.6564331054688\n",
            "EPOCHS: 17/400---------Loss: 740.0343627929688\n",
            "EPOCHS: 18/400---------Loss: 734.517822265625\n",
            "EPOCHS: 19/400---------Loss: 729.8458251953125\n",
            "EPOCHS: 20/400---------Loss: 725.8192138671875\n",
            "EPOCHS: 21/400---------Loss: 722.2867431640625\n",
            "EPOCHS: 22/400---------Loss: 719.1328125\n",
            "EPOCHS: 23/400---------Loss: 716.2694091796875\n",
            "EPOCHS: 24/400---------Loss: 713.6295166015625\n",
            "EPOCHS: 25/400---------Loss: 711.1622314453125\n",
            "EPOCHS: 26/400---------Loss: 708.8287963867188\n",
            "EPOCHS: 27/400---------Loss: 706.5989990234375\n",
            "EPOCHS: 28/400---------Loss: 704.4512939453125\n",
            "EPOCHS: 29/400---------Loss: 702.3675537109375\n",
            "EPOCHS: 30/400---------Loss: 700.3350830078125\n",
            "EPOCHS: 31/400---------Loss: 698.3438720703125\n",
            "EPOCHS: 32/400---------Loss: 696.3863525390625\n",
            "EPOCHS: 33/400---------Loss: 694.4564208984375\n",
            "EPOCHS: 34/400---------Loss: 692.5502319335938\n",
            "EPOCHS: 35/400---------Loss: 690.66357421875\n",
            "EPOCHS: 36/400---------Loss: 688.7942504882812\n",
            "EPOCHS: 37/400---------Loss: 686.9405517578125\n",
            "EPOCHS: 38/400---------Loss: 685.1004028320312\n",
            "EPOCHS: 39/400---------Loss: 683.2730712890625\n",
            "EPOCHS: 40/400---------Loss: 681.4571533203125\n",
            "EPOCHS: 41/400---------Loss: 679.6526489257812\n",
            "EPOCHS: 42/400---------Loss: 677.8583984375\n",
            "EPOCHS: 43/400---------Loss: 676.0742797851562\n",
            "EPOCHS: 44/400---------Loss: 674.2997436523438\n",
            "EPOCHS: 45/400---------Loss: 672.53466796875\n",
            "EPOCHS: 46/400---------Loss: 670.7791137695312\n",
            "EPOCHS: 47/400---------Loss: 669.0323486328125\n",
            "EPOCHS: 48/400---------Loss: 667.2943115234375\n",
            "EPOCHS: 49/400---------Loss: 665.5655517578125\n",
            "EPOCHS: 50/400---------Loss: 663.8450927734375\n",
            "EPOCHS: 51/400---------Loss: 662.1334228515625\n",
            "EPOCHS: 52/400---------Loss: 660.4299926757812\n",
            "EPOCHS: 53/400---------Loss: 658.7350463867188\n",
            "EPOCHS: 54/400---------Loss: 657.0485229492188\n",
            "EPOCHS: 55/400---------Loss: 655.3704833984375\n",
            "EPOCHS: 56/400---------Loss: 653.7003784179688\n",
            "EPOCHS: 57/400---------Loss: 652.0386962890625\n",
            "EPOCHS: 58/400---------Loss: 650.3849487304688\n",
            "EPOCHS: 59/400---------Loss: 648.7394409179688\n",
            "EPOCHS: 60/400---------Loss: 647.1019287109375\n",
            "EPOCHS: 61/400---------Loss: 645.4722900390625\n",
            "EPOCHS: 62/400---------Loss: 643.8507690429688\n",
            "EPOCHS: 63/400---------Loss: 642.2371215820312\n",
            "EPOCHS: 64/400---------Loss: 640.6312255859375\n",
            "EPOCHS: 65/400---------Loss: 639.0330810546875\n",
            "EPOCHS: 66/400---------Loss: 637.4427490234375\n",
            "EPOCHS: 67/400---------Loss: 635.8602294921875\n",
            "EPOCHS: 68/400---------Loss: 634.2852783203125\n",
            "EPOCHS: 69/400---------Loss: 632.7179565429688\n",
            "EPOCHS: 70/400---------Loss: 631.1583251953125\n",
            "EPOCHS: 71/400---------Loss: 629.6060791015625\n",
            "EPOCHS: 72/400---------Loss: 628.0615844726562\n",
            "EPOCHS: 73/400---------Loss: 626.5241088867188\n",
            "EPOCHS: 74/400---------Loss: 624.9943237304688\n",
            "EPOCHS: 75/400---------Loss: 623.4718627929688\n",
            "EPOCHS: 76/400---------Loss: 621.9567260742188\n",
            "EPOCHS: 77/400---------Loss: 620.4489135742188\n",
            "EPOCHS: 78/400---------Loss: 618.9482421875\n",
            "EPOCHS: 79/400---------Loss: 617.4549560546875\n",
            "EPOCHS: 80/400---------Loss: 615.96875\n",
            "EPOCHS: 81/400---------Loss: 614.489501953125\n",
            "EPOCHS: 82/400---------Loss: 613.0177001953125\n",
            "EPOCHS: 83/400---------Loss: 611.5525512695312\n",
            "EPOCHS: 84/400---------Loss: 610.0946044921875\n",
            "EPOCHS: 85/400---------Loss: 608.6435546875\n",
            "EPOCHS: 86/400---------Loss: 607.1995239257812\n",
            "EPOCHS: 87/400---------Loss: 605.7623291015625\n",
            "EPOCHS: 88/400---------Loss: 604.331787109375\n",
            "EPOCHS: 89/400---------Loss: 602.9083862304688\n",
            "EPOCHS: 90/400---------Loss: 601.4915771484375\n",
            "EPOCHS: 91/400---------Loss: 600.08154296875\n",
            "EPOCHS: 92/400---------Loss: 598.6781005859375\n",
            "EPOCHS: 93/400---------Loss: 597.2813720703125\n",
            "EPOCHS: 94/400---------Loss: 595.8914184570312\n",
            "EPOCHS: 95/400---------Loss: 594.5078125\n",
            "EPOCHS: 96/400---------Loss: 593.130859375\n",
            "EPOCHS: 97/400---------Loss: 591.7605590820312\n",
            "EPOCHS: 98/400---------Loss: 590.3966064453125\n",
            "EPOCHS: 99/400---------Loss: 589.0390625\n",
            "EPOCHS: 100/400---------Loss: 587.6878662109375\n",
            "EPOCHS: 101/400---------Loss: 586.3432006835938\n",
            "EPOCHS: 102/400---------Loss: 585.0046997070312\n",
            "EPOCHS: 103/400---------Loss: 583.6725463867188\n",
            "EPOCHS: 104/400---------Loss: 582.3467407226562\n",
            "EPOCHS: 105/400---------Loss: 581.0271606445312\n",
            "EPOCHS: 106/400---------Loss: 579.7135620117188\n",
            "EPOCHS: 107/400---------Loss: 578.4063720703125\n",
            "EPOCHS: 108/400---------Loss: 577.1051635742188\n",
            "EPOCHS: 109/400---------Loss: 575.8098754882812\n",
            "EPOCHS: 110/400---------Loss: 574.5208740234375\n",
            "EPOCHS: 111/400---------Loss: 573.2379150390625\n",
            "EPOCHS: 112/400---------Loss: 571.9608154296875\n",
            "EPOCHS: 113/400---------Loss: 570.6898193359375\n",
            "EPOCHS: 114/400---------Loss: 569.4246215820312\n",
            "EPOCHS: 115/400---------Loss: 568.1654052734375\n",
            "EPOCHS: 116/400---------Loss: 566.911865234375\n",
            "EPOCHS: 117/400---------Loss: 565.6643676757812\n",
            "EPOCHS: 118/400---------Loss: 564.42236328125\n",
            "EPOCHS: 119/400---------Loss: 563.1864013671875\n",
            "EPOCHS: 120/400---------Loss: 561.9561157226562\n",
            "EPOCHS: 121/400---------Loss: 560.7315673828125\n",
            "EPOCHS: 122/400---------Loss: 559.5125122070312\n",
            "EPOCHS: 123/400---------Loss: 558.2991333007812\n",
            "EPOCHS: 124/400---------Loss: 557.0914916992188\n",
            "EPOCHS: 125/400---------Loss: 555.88916015625\n",
            "EPOCHS: 126/400---------Loss: 554.6925048828125\n",
            "EPOCHS: 127/400---------Loss: 553.50146484375\n",
            "EPOCHS: 128/400---------Loss: 552.3157958984375\n",
            "EPOCHS: 129/400---------Loss: 551.1356201171875\n",
            "EPOCHS: 130/400---------Loss: 549.9608154296875\n",
            "EPOCHS: 131/400---------Loss: 548.7913818359375\n",
            "EPOCHS: 132/400---------Loss: 547.6273193359375\n",
            "EPOCHS: 133/400---------Loss: 546.4685668945312\n",
            "EPOCHS: 134/400---------Loss: 545.3151245117188\n",
            "EPOCHS: 135/400---------Loss: 544.1669921875\n",
            "EPOCHS: 136/400---------Loss: 543.0240478515625\n",
            "EPOCHS: 137/400---------Loss: 541.88623046875\n",
            "EPOCHS: 138/400---------Loss: 540.75390625\n",
            "EPOCHS: 139/400---------Loss: 539.62646484375\n",
            "EPOCHS: 140/400---------Loss: 538.5042724609375\n",
            "EPOCHS: 141/400---------Loss: 537.3870849609375\n",
            "EPOCHS: 142/400---------Loss: 536.27490234375\n",
            "EPOCHS: 143/400---------Loss: 535.16796875\n",
            "EPOCHS: 144/400---------Loss: 534.0660400390625\n",
            "EPOCHS: 145/400---------Loss: 532.9690551757812\n",
            "EPOCHS: 146/400---------Loss: 531.8768310546875\n",
            "EPOCHS: 147/400---------Loss: 530.789794921875\n",
            "EPOCHS: 148/400---------Loss: 529.70751953125\n",
            "EPOCHS: 149/400---------Loss: 528.6301879882812\n",
            "EPOCHS: 150/400---------Loss: 527.5574951171875\n",
            "EPOCHS: 151/400---------Loss: 526.4899291992188\n",
            "EPOCHS: 152/400---------Loss: 525.4271850585938\n",
            "EPOCHS: 153/400---------Loss: 524.3687744140625\n",
            "EPOCHS: 154/400---------Loss: 523.3155517578125\n",
            "EPOCHS: 155/400---------Loss: 522.2669677734375\n",
            "EPOCHS: 156/400---------Loss: 521.2229614257812\n",
            "EPOCHS: 157/400---------Loss: 520.1837158203125\n",
            "EPOCHS: 158/400---------Loss: 519.1489868164062\n",
            "EPOCHS: 159/400---------Loss: 518.1190185546875\n",
            "EPOCHS: 160/400---------Loss: 517.0936889648438\n",
            "EPOCHS: 161/400---------Loss: 516.0728759765625\n",
            "EPOCHS: 162/400---------Loss: 515.056396484375\n",
            "EPOCHS: 163/400---------Loss: 514.0446166992188\n",
            "EPOCHS: 164/400---------Loss: 513.0372314453125\n",
            "EPOCHS: 165/400---------Loss: 512.0345458984375\n",
            "EPOCHS: 166/400---------Loss: 511.0361328125\n",
            "EPOCHS: 167/400---------Loss: 510.0421447753906\n",
            "EPOCHS: 168/400---------Loss: 509.052490234375\n",
            "EPOCHS: 169/400---------Loss: 508.0673828125\n",
            "EPOCHS: 170/400---------Loss: 507.0865783691406\n",
            "EPOCHS: 171/400---------Loss: 506.11004638671875\n",
            "EPOCHS: 172/400---------Loss: 505.1378479003906\n",
            "EPOCHS: 173/400---------Loss: 504.16998291015625\n",
            "EPOCHS: 174/400---------Loss: 503.2062072753906\n",
            "EPOCHS: 175/400---------Loss: 502.2469177246094\n",
            "EPOCHS: 176/400---------Loss: 501.2915954589844\n",
            "EPOCHS: 177/400---------Loss: 500.34051513671875\n",
            "EPOCHS: 178/400---------Loss: 499.393798828125\n",
            "EPOCHS: 179/400---------Loss: 498.450927734375\n",
            "EPOCHS: 180/400---------Loss: 497.5123596191406\n",
            "EPOCHS: 181/400---------Loss: 496.577880859375\n",
            "EPOCHS: 182/400---------Loss: 495.6473693847656\n",
            "EPOCHS: 183/400---------Loss: 494.7210388183594\n",
            "EPOCHS: 184/400---------Loss: 493.79864501953125\n",
            "EPOCHS: 185/400---------Loss: 492.88037109375\n",
            "EPOCHS: 186/400---------Loss: 491.96612548828125\n",
            "EPOCHS: 187/400---------Loss: 491.0556640625\n",
            "EPOCHS: 188/400---------Loss: 490.14910888671875\n",
            "EPOCHS: 189/400---------Loss: 489.2467346191406\n",
            "EPOCHS: 190/400---------Loss: 488.34820556640625\n",
            "EPOCHS: 191/400---------Loss: 487.45343017578125\n",
            "EPOCHS: 192/400---------Loss: 486.56268310546875\n",
            "EPOCHS: 193/400---------Loss: 485.67547607421875\n",
            "EPOCHS: 194/400---------Loss: 484.7923889160156\n",
            "EPOCHS: 195/400---------Loss: 483.9129943847656\n",
            "EPOCHS: 196/400---------Loss: 483.03729248046875\n",
            "EPOCHS: 197/400---------Loss: 482.1656188964844\n",
            "EPOCHS: 198/400---------Loss: 481.2974548339844\n",
            "EPOCHS: 199/400---------Loss: 480.43304443359375\n",
            "EPOCHS: 200/400---------Loss: 479.57244873046875\n",
            "EPOCHS: 201/400---------Loss: 478.71533203125\n",
            "EPOCHS: 202/400---------Loss: 477.86212158203125\n",
            "EPOCHS: 203/400---------Loss: 477.0122985839844\n",
            "EPOCHS: 204/400---------Loss: 476.1663513183594\n",
            "EPOCHS: 205/400---------Loss: 475.3238220214844\n",
            "EPOCHS: 206/400---------Loss: 474.4849548339844\n",
            "EPOCHS: 207/400---------Loss: 473.649658203125\n",
            "EPOCHS: 208/400---------Loss: 472.81793212890625\n",
            "EPOCHS: 209/400---------Loss: 471.989501953125\n",
            "EPOCHS: 210/400---------Loss: 471.1649475097656\n",
            "EPOCHS: 211/400---------Loss: 470.3436584472656\n",
            "EPOCHS: 212/400---------Loss: 469.52593994140625\n",
            "EPOCHS: 213/400---------Loss: 468.7115783691406\n",
            "EPOCHS: 214/400---------Loss: 467.9007263183594\n",
            "EPOCHS: 215/400---------Loss: 467.09326171875\n",
            "EPOCHS: 216/400---------Loss: 466.28912353515625\n",
            "EPOCHS: 217/400---------Loss: 465.48858642578125\n",
            "EPOCHS: 218/400---------Loss: 464.691162109375\n",
            "EPOCHS: 219/400---------Loss: 463.89727783203125\n",
            "EPOCHS: 220/400---------Loss: 463.1065979003906\n",
            "EPOCHS: 221/400---------Loss: 462.3191833496094\n",
            "EPOCHS: 222/400---------Loss: 461.5352478027344\n",
            "EPOCHS: 223/400---------Loss: 460.75439453125\n",
            "EPOCHS: 224/400---------Loss: 459.9769592285156\n",
            "EPOCHS: 225/400---------Loss: 459.20263671875\n",
            "EPOCHS: 226/400---------Loss: 458.431640625\n",
            "EPOCHS: 227/400---------Loss: 457.663818359375\n",
            "EPOCHS: 228/400---------Loss: 456.89923095703125\n",
            "EPOCHS: 229/400---------Loss: 456.1376953125\n",
            "EPOCHS: 230/400---------Loss: 455.37939453125\n",
            "EPOCHS: 231/400---------Loss: 454.62432861328125\n",
            "EPOCHS: 232/400---------Loss: 453.8722229003906\n",
            "EPOCHS: 233/400---------Loss: 453.12335205078125\n",
            "EPOCHS: 234/400---------Loss: 452.37738037109375\n",
            "EPOCHS: 235/400---------Loss: 451.6346740722656\n",
            "EPOCHS: 236/400---------Loss: 450.8949279785156\n",
            "EPOCHS: 237/400---------Loss: 450.1582946777344\n",
            "EPOCHS: 238/400---------Loss: 449.4247131347656\n",
            "EPOCHS: 239/400---------Loss: 448.69403076171875\n",
            "EPOCHS: 240/400---------Loss: 447.96636962890625\n",
            "EPOCHS: 241/400---------Loss: 447.2417907714844\n",
            "EPOCHS: 242/400---------Loss: 446.5201721191406\n",
            "EPOCHS: 243/400---------Loss: 445.8014221191406\n",
            "EPOCHS: 244/400---------Loss: 445.0856018066406\n",
            "EPOCHS: 245/400---------Loss: 444.3726501464844\n",
            "EPOCHS: 246/400---------Loss: 443.6627502441406\n",
            "EPOCHS: 247/400---------Loss: 442.95562744140625\n",
            "EPOCHS: 248/400---------Loss: 442.25146484375\n",
            "EPOCHS: 249/400---------Loss: 441.54998779296875\n",
            "EPOCHS: 250/400---------Loss: 440.85162353515625\n",
            "EPOCHS: 251/400---------Loss: 440.1558532714844\n",
            "EPOCHS: 252/400---------Loss: 439.46307373046875\n",
            "EPOCHS: 253/400---------Loss: 438.77294921875\n",
            "EPOCHS: 254/400---------Loss: 438.08563232421875\n",
            "EPOCHS: 255/400---------Loss: 437.4012756347656\n",
            "EPOCHS: 256/400---------Loss: 436.719482421875\n",
            "EPOCHS: 257/400---------Loss: 436.0404357910156\n",
            "EPOCHS: 258/400---------Loss: 435.3642578125\n",
            "EPOCHS: 259/400---------Loss: 434.69061279296875\n",
            "EPOCHS: 260/400---------Loss: 434.01983642578125\n",
            "EPOCHS: 261/400---------Loss: 433.35162353515625\n",
            "EPOCHS: 262/400---------Loss: 432.6861877441406\n",
            "EPOCHS: 263/400---------Loss: 432.0232849121094\n",
            "EPOCHS: 264/400---------Loss: 431.36309814453125\n",
            "EPOCHS: 265/400---------Loss: 430.70556640625\n",
            "EPOCHS: 266/400---------Loss: 430.0506896972656\n",
            "EPOCHS: 267/400---------Loss: 429.3982849121094\n",
            "EPOCHS: 268/400---------Loss: 428.7486267089844\n",
            "EPOCHS: 269/400---------Loss: 428.10137939453125\n",
            "EPOCHS: 270/400---------Loss: 427.4568786621094\n",
            "EPOCHS: 271/400---------Loss: 426.8147888183594\n",
            "EPOCHS: 272/400---------Loss: 426.17529296875\n",
            "EPOCHS: 273/400---------Loss: 425.5384216308594\n",
            "EPOCHS: 274/400---------Loss: 424.9039001464844\n",
            "EPOCHS: 275/400---------Loss: 424.27191162109375\n",
            "EPOCHS: 276/400---------Loss: 423.6424865722656\n",
            "EPOCHS: 277/400---------Loss: 423.0155334472656\n",
            "EPOCHS: 278/400---------Loss: 422.3909606933594\n",
            "EPOCHS: 279/400---------Loss: 421.7689514160156\n",
            "EPOCHS: 280/400---------Loss: 421.1493225097656\n",
            "EPOCHS: 281/400---------Loss: 420.5320739746094\n",
            "EPOCHS: 282/400---------Loss: 419.91729736328125\n",
            "EPOCHS: 283/400---------Loss: 419.304931640625\n",
            "EPOCHS: 284/400---------Loss: 418.6949157714844\n",
            "EPOCHS: 285/400---------Loss: 418.08721923828125\n",
            "EPOCHS: 286/400---------Loss: 417.4820251464844\n",
            "EPOCHS: 287/400---------Loss: 416.87908935546875\n",
            "EPOCHS: 288/400---------Loss: 416.2784729003906\n",
            "EPOCHS: 289/400---------Loss: 415.6802673339844\n",
            "EPOCHS: 290/400---------Loss: 415.08447265625\n",
            "EPOCHS: 291/400---------Loss: 414.49078369140625\n",
            "EPOCHS: 292/400---------Loss: 413.8995056152344\n",
            "EPOCHS: 293/400---------Loss: 413.31048583984375\n",
            "EPOCHS: 294/400---------Loss: 412.7237243652344\n",
            "EPOCHS: 295/400---------Loss: 412.13922119140625\n",
            "EPOCHS: 296/400---------Loss: 411.55694580078125\n",
            "EPOCHS: 297/400---------Loss: 410.9769592285156\n",
            "EPOCHS: 298/400---------Loss: 410.39910888671875\n",
            "EPOCHS: 299/400---------Loss: 409.82354736328125\n",
            "EPOCHS: 300/400---------Loss: 409.25018310546875\n",
            "EPOCHS: 301/400---------Loss: 408.6789855957031\n",
            "EPOCHS: 302/400---------Loss: 408.1100158691406\n",
            "EPOCHS: 303/400---------Loss: 407.543212890625\n",
            "EPOCHS: 304/400---------Loss: 406.9785461425781\n",
            "EPOCHS: 305/400---------Loss: 406.41607666015625\n",
            "EPOCHS: 306/400---------Loss: 405.8556823730469\n",
            "EPOCHS: 307/400---------Loss: 405.29742431640625\n",
            "EPOCHS: 308/400---------Loss: 404.7413635253906\n",
            "EPOCHS: 309/400---------Loss: 404.1872863769531\n",
            "EPOCHS: 310/400---------Loss: 403.63543701171875\n",
            "EPOCHS: 311/400---------Loss: 403.0856018066406\n",
            "EPOCHS: 312/400---------Loss: 402.53790283203125\n",
            "EPOCHS: 313/400---------Loss: 401.9920654296875\n",
            "EPOCHS: 314/400---------Loss: 401.448486328125\n",
            "EPOCHS: 315/400---------Loss: 400.90692138671875\n",
            "EPOCHS: 316/400---------Loss: 400.36737060546875\n",
            "EPOCHS: 317/400---------Loss: 399.8299255371094\n",
            "EPOCHS: 318/400---------Loss: 399.2944030761719\n",
            "EPOCHS: 319/400---------Loss: 398.7608947753906\n",
            "EPOCHS: 320/400---------Loss: 398.2294006347656\n",
            "EPOCHS: 321/400---------Loss: 397.69989013671875\n",
            "EPOCHS: 322/400---------Loss: 397.1722717285156\n",
            "EPOCHS: 323/400---------Loss: 396.646728515625\n",
            "EPOCHS: 324/400---------Loss: 396.1231994628906\n",
            "EPOCHS: 325/400---------Loss: 395.6014709472656\n",
            "EPOCHS: 326/400---------Loss: 395.0816955566406\n",
            "EPOCHS: 327/400---------Loss: 394.56390380859375\n",
            "EPOCHS: 328/400---------Loss: 394.04803466796875\n",
            "EPOCHS: 329/400---------Loss: 393.5340881347656\n",
            "EPOCHS: 330/400---------Loss: 393.0220031738281\n",
            "EPOCHS: 331/400---------Loss: 392.51177978515625\n",
            "EPOCHS: 332/400---------Loss: 392.0035095214844\n",
            "EPOCHS: 333/400---------Loss: 391.4969787597656\n",
            "EPOCHS: 334/400---------Loss: 390.99237060546875\n",
            "EPOCHS: 335/400---------Loss: 390.4897766113281\n",
            "EPOCHS: 336/400---------Loss: 389.98883056640625\n",
            "EPOCHS: 337/400---------Loss: 389.48980712890625\n",
            "EPOCHS: 338/400---------Loss: 388.99261474609375\n",
            "EPOCHS: 339/400---------Loss: 388.497314453125\n",
            "EPOCHS: 340/400---------Loss: 388.00372314453125\n",
            "EPOCHS: 341/400---------Loss: 387.511962890625\n",
            "EPOCHS: 342/400---------Loss: 387.02191162109375\n",
            "EPOCHS: 343/400---------Loss: 386.53369140625\n",
            "EPOCHS: 344/400---------Loss: 386.04730224609375\n",
            "EPOCHS: 345/400---------Loss: 385.562744140625\n",
            "EPOCHS: 346/400---------Loss: 385.07977294921875\n",
            "EPOCHS: 347/400---------Loss: 384.59857177734375\n",
            "EPOCHS: 348/400---------Loss: 384.119140625\n",
            "EPOCHS: 349/400---------Loss: 383.64154052734375\n",
            "EPOCHS: 350/400---------Loss: 383.1656494140625\n",
            "EPOCHS: 351/400---------Loss: 382.69134521484375\n",
            "EPOCHS: 352/400---------Loss: 382.21881103515625\n",
            "EPOCHS: 353/400---------Loss: 381.7478942871094\n",
            "EPOCHS: 354/400---------Loss: 381.27880859375\n",
            "EPOCHS: 355/400---------Loss: 380.8113708496094\n",
            "EPOCHS: 356/400---------Loss: 380.3455505371094\n",
            "EPOCHS: 357/400---------Loss: 379.8815002441406\n",
            "EPOCHS: 358/400---------Loss: 379.41900634765625\n",
            "EPOCHS: 359/400---------Loss: 378.95831298828125\n",
            "EPOCHS: 360/400---------Loss: 378.4991760253906\n",
            "EPOCHS: 361/400---------Loss: 378.0416564941406\n",
            "EPOCHS: 362/400---------Loss: 377.58563232421875\n",
            "EPOCHS: 363/400---------Loss: 377.1314392089844\n",
            "EPOCHS: 364/400---------Loss: 376.6787109375\n",
            "EPOCHS: 365/400---------Loss: 376.2277526855469\n",
            "EPOCHS: 366/400---------Loss: 375.77825927734375\n",
            "EPOCHS: 367/400---------Loss: 375.3303527832031\n",
            "EPOCHS: 368/400---------Loss: 374.88409423828125\n",
            "EPOCHS: 369/400---------Loss: 374.43939208984375\n",
            "EPOCHS: 370/400---------Loss: 373.99627685546875\n",
            "EPOCHS: 371/400---------Loss: 373.55462646484375\n",
            "EPOCHS: 372/400---------Loss: 373.11468505859375\n",
            "EPOCHS: 373/400---------Loss: 372.6761779785156\n",
            "EPOCHS: 374/400---------Loss: 372.23919677734375\n",
            "EPOCHS: 375/400---------Loss: 371.80377197265625\n",
            "EPOCHS: 376/400---------Loss: 371.369873046875\n",
            "EPOCHS: 377/400---------Loss: 370.9375\n",
            "EPOCHS: 378/400---------Loss: 370.5066833496094\n",
            "EPOCHS: 379/400---------Loss: 370.0773010253906\n",
            "EPOCHS: 380/400---------Loss: 369.6494140625\n",
            "EPOCHS: 381/400---------Loss: 369.2230529785156\n",
            "EPOCHS: 382/400---------Loss: 368.798095703125\n",
            "EPOCHS: 383/400---------Loss: 368.3747253417969\n",
            "EPOCHS: 384/400---------Loss: 367.9527282714844\n",
            "EPOCHS: 385/400---------Loss: 367.5322570800781\n",
            "EPOCHS: 386/400---------Loss: 367.11322021484375\n",
            "EPOCHS: 387/400---------Loss: 366.6956481933594\n",
            "EPOCHS: 388/400---------Loss: 366.2794494628906\n",
            "EPOCHS: 389/400---------Loss: 365.86474609375\n",
            "EPOCHS: 390/400---------Loss: 365.4513244628906\n",
            "EPOCHS: 391/400---------Loss: 365.03961181640625\n",
            "EPOCHS: 392/400---------Loss: 364.6290588378906\n",
            "EPOCHS: 393/400---------Loss: 364.22003173828125\n",
            "EPOCHS: 394/400---------Loss: 363.8123474121094\n",
            "EPOCHS: 395/400---------Loss: 363.4060974121094\n",
            "EPOCHS: 396/400---------Loss: 363.001220703125\n",
            "EPOCHS: 397/400---------Loss: 362.59771728515625\n",
            "EPOCHS: 398/400---------Loss: 362.19561767578125\n",
            "EPOCHS: 399/400---------Loss: 361.7948303222656\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preds = model(inputs)\n",
        "preds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cKWxdlNCg4a_",
        "outputId": "d22c4298-e702-45db-ead4-0b34f83960fd"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 39.8369,  64.1509],\n",
              "        [ 55.8679,  45.4663],\n",
              "        [ 81.7229,  74.5094],\n",
              "        [ 28.0953,  58.2042],\n",
              "        [110.9677,  75.2111]], grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "targets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3hFfA9OhXUk",
        "outputId": "dcee104d-f5c1-4f86-f800-4c881d8b5c72"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 32.,  66.],\n",
              "        [ 52.,  76.],\n",
              "        [102.,  55.],\n",
              "        [ 32.,  26.],\n",
              "        [ 99., 100.]])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using Torch built in function\n"
      ],
      "metadata": {
        "id": "hb4mZZJE3b0N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using Torch built in function\n",
        "import torch.nn as nn\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "HDQ7zyzdhaKL"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Input (temp, rainfall, humidity)\n",
        "inputs = np.array([[73, 67, 43],\n",
        "                   [91, 88, 64],\n",
        "                   [87, 134, 58],\n",
        "                   [102, 43, 37],\n",
        "                   [69, 96, 70],\n",
        "                   [74, 66, 43],\n",
        "                   [91, 87, 65],\n",
        "                   [88, 134, 59],\n",
        "                   [101, 44, 37],\n",
        "                   [68, 96, 71],\n",
        "                   [73, 66, 44],\n",
        "                   [92, 87, 64],\n",
        "                   [87, 135, 57],\n",
        "                   [103, 43, 36],\n",
        "                   [68, 97, 70]],\n",
        "                  dtype='float32')\n",
        "\n",
        "# Targets (apples, oranges)\n",
        "targets = np.array([[56, 70],\n",
        "                    [81, 101],\n",
        "                    [119, 133],\n",
        "                    [22, 37],\n",
        "                    [103, 119],\n",
        "                    [57, 69],\n",
        "                    [80, 102],\n",
        "                    [118, 132],\n",
        "                    [21, 38],\n",
        "                    [104, 118],\n",
        "                    [57, 69],\n",
        "                    [82, 100],\n",
        "                    [118, 134],\n",
        "                    [20, 38],\n",
        "                    [102, 120]],\n",
        "                   dtype='float32')\n",
        "\n",
        "inputs = torch.from_numpy(inputs)\n",
        "targets = torch.from_numpy(targets)\n"
      ],
      "metadata": {
        "id": "Hk2NE2Hb32An"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lgmbZi0U6N1W",
        "outputId": "28999b94-9914-47a1-a65e-d2241db6017e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 73.,  67.,  43.],\n",
              "        [ 91.,  88.,  64.],\n",
              "        [ 87., 134.,  58.],\n",
              "        [102.,  43.,  37.],\n",
              "        [ 69.,  96.,  70.],\n",
              "        [ 74.,  66.,  43.],\n",
              "        [ 91.,  87.,  65.],\n",
              "        [ 88., 134.,  59.],\n",
              "        [101.,  44.,  37.],\n",
              "        [ 68.,  96.,  71.],\n",
              "        [ 73.,  66.,  44.],\n",
              "        [ 92.,  87.,  64.],\n",
              "        [ 87., 135.,  57.],\n",
              "        [103.,  43.,  36.],\n",
              "        [ 68.,  97.,  70.]])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are using 15 training examples to illustrate how to work with large datasets in small batches.\n",
        "\n",
        "# Dataset and DataLoader\n",
        "We'll create a TensorDataset, which allows access to rows from inputs and targets as tuples, and provides standard APIs for working with many different types of datasets in PyTorch."
      ],
      "metadata": {
        "id": "z6s1U7kak5fN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset"
      ],
      "metadata": {
        "id": "794jCXqE6eRh"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define dataset\n",
        "train_ds = TensorDataset(inputs, targets)\n",
        "train_ds[0:3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IorETMFUltNi",
        "outputId": "aa58a5e7-8e60-4f1f-fc89-825d8c843419"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[ 73.,  67.,  43.],\n",
              "         [ 91.,  88.,  64.],\n",
              "         [ 87., 134.,  58.]]),\n",
              " tensor([[ 56.,  70.],\n",
              "         [ 81., 101.],\n",
              "         [119., 133.]]))"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The TensorDataset allows us to access a small section of the training data using the array indexing notation ([0:3] in the above code). It returns a tuple with two elements. The first element contains the input variables for the selected rows, and the second contains the targets.\n",
        "\n",
        "We'll also create a DataLoader, which can split the data into batches of a predefined size while training. It also provides other utilities like shuffling and random sampling of the data."
      ],
      "metadata": {
        "id": "EDvdJaX3mX3G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader"
      ],
      "metadata": {
        "id": "agevv9i5lxFF"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define data loader\n",
        "batch_size = 5\n",
        "train_dl = DataLoader(train_ds, batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "ifupzZD0n2qc"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use the data loader in a for loop. Let's look at an example."
      ],
      "metadata": {
        "id": "eKy602VVoZCT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for xb, yb in train_dl:\n",
        "    print(xb)\n",
        "    print(yb)\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H5TmgGOPn_sw",
        "outputId": "eb8db2d8-47f2-4964-b9b2-a07db669af8e"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[101.,  44.,  37.],\n",
            "        [ 68.,  96.,  71.],\n",
            "        [ 69.,  96.,  70.],\n",
            "        [102.,  43.,  37.],\n",
            "        [ 68.,  97.,  70.]])\n",
            "tensor([[ 21.,  38.],\n",
            "        [104., 118.],\n",
            "        [103., 119.],\n",
            "        [ 22.,  37.],\n",
            "        [102., 120.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In each iteration, the data loader returns one batch of data with the given batch size. If shuffle is set to True, it shuffles the training data before creating batches. Shuffling helps randomize the input to the optimization algorithm, leading to a faster reduction in the loss.\n",
        "\n",
        "#nn.Linear\n",
        "Instead of initializing the weights & biases manually, we can define the model using the nn.Linear class from PyTorch, which does it automatically."
      ],
      "metadata": {
        "id": "1Ig6owi4tldJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#nn.Linear\n",
        "# Define model\n",
        "model = nn.Linear(3, 2) # 3 features 2 outputs\n",
        "print(model.weight)\n",
        "print(model.bias)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tq3U1Y-qoc-z",
        "outputId": "531080e4-fa7b-436b-fe4f-4512fe2b489c"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[-0.2414,  0.2400, -0.5716],\n",
            "        [-0.2486, -0.1453, -0.1655]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([0.0337, 0.1024], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PyTorch models also have a helpful .parameters method, which returns a list containing all the weights and bias matrices present in the model. For our linear regression model, we have one weight matrix and one bias matrix."
      ],
      "metadata": {
        "id": "XB_yb8D_vBAi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "list(model.parameters())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cXYDOwgMtujg",
        "outputId": "f0a0602a-c8e1-4406-8931-8ba59700aeab"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Parameter containing:\n",
              " tensor([[-0.2414,  0.2400, -0.5716],\n",
              "         [-0.2486, -0.1453, -0.1655]], requires_grad=True),\n",
              " Parameter containing:\n",
              " tensor([0.0337, 0.1024], requires_grad=True)]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate predictions\n",
        "preds = model(inputs)\n",
        "preds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AH-wAXNsvEzM",
        "outputId": "553ed7b1-250f-4cbb-d546-4482c1069423"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-26.0878, -34.8921],\n",
              "        [-37.3964, -45.8925],\n",
              "        [-21.9617, -50.5878],\n",
              "        [-35.4197, -37.6205],\n",
              "        [-33.5944, -42.5795],\n",
              "        [-26.5693, -34.9954],\n",
              "        [-38.2079, -45.9127],\n",
              "        [-22.7747, -51.0019],\n",
              "        [-34.9382, -37.5172],\n",
              "        [-33.9245, -42.4965],\n",
              "        [-26.8994, -34.9123],\n",
              "        [-37.8778, -45.9957],\n",
              "        [-21.1501, -50.5676],\n",
              "        [-35.0895, -37.7036],\n",
              "        [-33.1129, -42.4763]], grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loss Function\n",
        "Instead of defining a loss function manually, we can use the built-in loss function mse_loss."
      ],
      "metadata": {
        "id": "HopFz9iFwi9P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "2EEdc3QWvPh_"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The nn.functional package contains many useful loss functions and several other utilities."
      ],
      "metadata": {
        "id": "K5_SdmdswpIO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define loss function\n",
        "loss_fn = F.mse_loss"
      ],
      "metadata": {
        "id": "JeqHTlCIwpwE"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss = loss_fn(model(inputs), targets)\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-fgQLxzlw_Hy",
        "outputId": "363a935e-a398-4faa-f692-b3cc3ba4d31e"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(16049.4512, grad_fn=<MseLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Optimizer\n",
        "Instead of manually manipulating the model's weights & biases using gradients, we can use the optimizer optim.SGD. SGD is short for \"stochastic gradient descent\". The term stochastic indicates that samples are selected in random batches instead of as a single group."
      ],
      "metadata": {
        "id": "i4Bc_7jqxZRa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# optimizer\n",
        "opt = torch.optim.SGD(model.parameters(), lr = 1e-5)"
      ],
      "metadata": {
        "id": "TvBzKAchxJls"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that model.parameters() is passed as an argument to optim.SGD so that the optimizer knows which matrices should be modified during the update step. Also, we can specify a learning rate that controls the amount by which the parameters are modified."
      ],
      "metadata": {
        "id": "CkPTLsIoa88C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train the model\n",
        "We are now ready to train the model. We'll follow the same process to implement gradient descent:\n",
        "\n",
        "1. Generate predictions\n",
        "\n",
        "2. Calculate the loss\n",
        "\n",
        "3. Compute gradients w.r.t the weights and biases\n",
        "\n",
        "4. Adjust the weights by subtracting a small quantity proportional to the gradient\n",
        "\n",
        "5. Reset the gradients to zero\n",
        "\n",
        "The only change is that we'll work batches of data instead of processing the entire training data in every iteration. Let's define a utility function fit that trains the model for a given number of epochs."
      ],
      "metadata": {
        "id": "sZFoP36qbULw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Utility function to train the model\n",
        "def fit(num_epochs, model, loss_fn, opt, train_dl):\n",
        "\n",
        "  # Repeat for given number of epochs\n",
        "  for epoch in range(num_epochs):\n",
        "\n",
        "    #train with batches of data\n",
        "    for xb,yb in train_dl:\n",
        "\n",
        "      #1. Generate predictions\n",
        "      pred = model(xb)\n",
        "\n",
        "      #2. Calculate loss\n",
        "      loss = loss_fn(pred, yb)\n",
        "\n",
        "      #3. Compute gradients\n",
        "      loss.backward()\n",
        "\n",
        "      #4. Update parameters using  gradients\n",
        "      opt.step()\n",
        "\n",
        "      #5. Reset the gradients to zero\n",
        "      opt.zero_grad()\n",
        "\n",
        "    # Print the progress\n",
        "    if (epoch+1) % 10 == 0:\n",
        "        print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))"
      ],
      "metadata": {
        "id": "VKa_B_zn854Y"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some things to note above:\n",
        "\n",
        "* We use the data loader defined earlier to get batches of data for every iteration.\n",
        "\n",
        "* Instead of updating parameters (weights and biases) manually, we use opt.step to perform the update and opt.zero_grad to reset the gradients to zero.\n",
        "\n",
        "* We've also added a log statement that prints the loss from the last batch of data for every 10th epoch to track training progress. loss.item returns the actual value stored in the loss tensor.\n",
        "\n",
        "Let's train the model for 100 epochs."
      ],
      "metadata": {
        "id": "NHbd8ml6fwBj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fit(100, model, loss_fn, opt, train_dl)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y58a8vs_fvF3",
        "outputId": "6330b6a1-3962-400d-b09b-22fa4610f63d"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/100], Loss: 360.1072\n",
            "Epoch [20/100], Loss: 89.0761\n",
            "Epoch [30/100], Loss: 277.6465\n",
            "Epoch [40/100], Loss: 162.6654\n",
            "Epoch [50/100], Loss: 103.6954\n",
            "Epoch [60/100], Loss: 84.9424\n",
            "Epoch [70/100], Loss: 92.9008\n",
            "Epoch [80/100], Loss: 40.3582\n",
            "Epoch [90/100], Loss: 67.7592\n",
            "Epoch [100/100], Loss: 89.8794\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's generate predictions using our model and verify that they're close to our targets.\n",
        "\n"
      ],
      "metadata": {
        "id": "m9fpxOoYf_3h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate predictions\n",
        "preds = model(inputs)\n",
        "preds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yftzKdp4f7Rc",
        "outputId": "79d17c6d-0c47-4d93-92ce-64d68ccc6ce2"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 58.5905,  71.7957],\n",
              "        [ 78.1540,  98.2699],\n",
              "        [126.7458, 136.9998],\n",
              "        [ 27.9296,  44.4373],\n",
              "        [ 90.7992, 110.5816],\n",
              "        [ 57.3616,  70.7939],\n",
              "        [ 77.1504,  97.8388],\n",
              "        [126.6053, 137.3522],\n",
              "        [ 29.1584,  45.4391],\n",
              "        [ 91.0245, 111.1523],\n",
              "        [ 57.5868,  71.3646],\n",
              "        [ 76.9251,  97.2681],\n",
              "        [127.7494, 137.4309],\n",
              "        [ 27.7043,  43.8666],\n",
              "        [ 92.0281, 111.5834]], grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare with targets\n",
        "targets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5RmT6FPDgD-g",
        "outputId": "f627e6fb-49d9-4bab-c35e-f5121840c161"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 56.,  70.],\n",
              "        [ 81., 101.],\n",
              "        [119., 133.],\n",
              "        [ 22.,  37.],\n",
              "        [103., 119.],\n",
              "        [ 57.,  69.],\n",
              "        [ 80., 102.],\n",
              "        [118., 132.],\n",
              "        [ 21.,  38.],\n",
              "        [104., 118.],\n",
              "        [ 57.,  69.],\n",
              "        [ 82., 100.],\n",
              "        [118., 134.],\n",
              "        [ 20.,  38.],\n",
              "        [102., 120.]])"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Indeed, the predictions are quite close to our targets. We have a trained a reasonably good model to predict crop yields for apples and oranges by looking at the average temperature, rainfall, and humidity in a region. We can use it to make predictions of crop yields for new regions by passing a batch containing a single row of input."
      ],
      "metadata": {
        "id": "oX67DdVVhMUg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model(torch.tensor([[75, 63, 44.]]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6s_lIs-ihJOO",
        "outputId": "bac24fb1-1f32-4d14-eebc-9fb333f50edd"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[54.0832, 68.4683]], grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The predicted yield of apples is 54.3 tons per hectare, and that of oranges is 68.3 tons per hectare."
      ],
      "metadata": {
        "id": "su0T7A9AhWc5"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oVOdD7CHhT-V"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}